#1. Load and Explore the Data
import pandas as pd
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
housing = fetch_california_housing()
data = pd.DataFrame(data=housing.data, columns=housing.feature_names)
data['target'] = housing.target

# Display the first few rows of the dataset
print(data.head())

#2.Create new features
data['Income_Age_Interaction'] = data['MedInc'] * data['HouseAge']
data['HouseAge_PerCap'] = data['HouseAge'] / data['Population']

# Display the updated dataframe with new features
print(data.head())

#3.Feature Selection
#a. Feature Importance with Random Forest

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Prepare features and target variable
X = data.drop('target', axis=1)
y = data['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importances
importances = rf.feature_importances_

# Create a DataFrame for feature importance
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

#b. Dimensionality Reduction with PCA
# b. Dimensionality Reduction with PCA
from sklearn.decomposition import PCA

# Initialize PCA
pca = PCA(n_components=2)  # Reducing to 2 dimensions for visualization

# Fit and transform the data
X_pca = pca.fit_transform(X)

# Create a DataFrame for the PCA results
pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
pca_df['target'] = y.values

print(pca_df.head())

#4. Evaluate Model Performance with Selected Features
from sklearn.metrics import mean_squared_error
import numpy as np

# Select top features based on importance
top_features = feature_importance_df['Feature'].head(5).tolist()  # Choose top 5 features
X_top_features = X[top_features]

# Split the data
X_train_top, X_test_top, y_train, y_test = train_test_split(X_top_features, y, test_size=0.3, random_state=42)

# Train model with top features
rf_top = RandomForestRegressor(n_estimators=100, random_state=42)
rf_top.fit(X_train_top, y_train)

# Make predictions and evaluate
y_pred = rf_top.predict(X_test_top)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"Model RMSE with top features: {rmse:.2f}")
